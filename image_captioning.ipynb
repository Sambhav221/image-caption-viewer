{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cce7750-445a-43eb-8c0c-1f3244a6dfd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2782781640.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    Display a collection of local images with their respective captions from a file. Useful for reviewing datasets, galleries, or annotations.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# üì∏ Image Caption Viewer Project\n",
    "\n",
    "## ‚úÖ Objective\n",
    "Display a collection of local images with their respective captions from a file. Useful for reviewing datasets, galleries, or annotations.\n",
    "\n",
    "## üìÅ Folder Structure\n",
    "- `images/`: contains all the image files\n",
    "- `captions.txt`: contains image filename + caption in format `filename.jpg|caption`\n",
    "- `image_viewer.ipynb`: this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f71ca-d518-47c6-b9ed-79c939d51928",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b41d26-c08d-42e7-991e-79b8b91e014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecf61d-2f11-410f-8d69-7c6e7c9d6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98a793-6b52-4de1-98e8-f4ba2ece565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the uploaded image\n",
    "image = cv2.imread(\"photo_2025-06-09_21-10-49.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize for model input\n",
    "image_resized = cv2.resize(image, (256, 256)) / 255.0\n",
    "image_resized = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "# Show the original image\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151a69d-3861-4110-8c62-46c5a1b77c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095088c3-cc71-439e-9cec-7fefdd801183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e99424-82f3-4e0b-9da0-c55ef3f057a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5771e7-5bd7-4405-b0cd-f03ebc5abb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pretrained DeepLabV3 model\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c1b33-8a45-4fd3-9480-28228b6ef05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e9e25-5553-4a79-956f-de5fec631a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights (recommended way)\n",
    "weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "\n",
    "# Load the model with these weights and set to eval mode\n",
    "model = deeplabv3_resnet101(weights=weights)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0560f0-01e8-4c66-b9c7-82107a08629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your image (replace 'your_image.jpg' with your file)\n",
    "img_path = 'photo_2025-06-09_21-10-49.jpg'\n",
    "input_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Get the preprocessing transform from the weights metadata\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Preprocess the image: resize, normalize, convert to tensor, etc.\n",
    "input_tensor = preprocess(input_image)\n",
    "\n",
    "# The model expects a batch, so add a batch dimension\n",
    "input_batch = input_tensor.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66387026-4199-46c9-9d5f-f95f4cfc4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    output = model(input_batch)['out'][0]  # Output is a dict with key 'out'\n",
    "\n",
    "# The output has shape [num_classes, H, W]\n",
    "print(output.shape)  # e.g., torch.Size([21, 480, 480])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c118cbe-7afd-445d-a0f0-91a543ca8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class per pixel by taking argmax\n",
    "output_predictions = output.argmax(0).byte().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7dc1c-59cc-4fc4-9459-ca451349f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class colors from the pretrained weights metadata\n",
    "palette = weights.meta[\"palette\"]\n",
    "# Palette is a list of RGB triplets for each class\n",
    "\n",
    "# Create a color mask using the palette\n",
    "color_mask = np.zeros((output_predictions.shape[0], output_predictions.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "for label, color in enumerate(palette):\n",
    "    color_mask[output_predictions == label] = color\n",
    "\n",
    "# Plot original image and mask overlay\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(input_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Segmentation Mask\")\n",
    "plt.imshow(color_mask)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(input_image)\n",
    "plt.imshow(color_mask, alpha=0.5)  # overlay with transparency\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4af9e-f2c6-46a9-adc9-23f64c087483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC color palette for 21 classes (including background)\n",
    "palette = [\n",
    "    (0, 0, 0),        # 0=background\n",
    "    (128, 0, 0),      # 1=aeroplane\n",
    "    (0, 128, 0),      # 2=bicycle\n",
    "    (128, 128, 0),    # 3=bird\n",
    "    (0, 0, 128),      # 4=boat\n",
    "    (128, 0, 128),    # 5=bottle\n",
    "    (0, 128, 128),    # 6=bus\n",
    "    (128, 128, 128),  # 7=car\n",
    "    (64, 0, 0),       # 8=cat\n",
    "    (192, 0, 0),      # 9=chair\n",
    "    (64, 128, 0),     # 10=cow\n",
    "    (192, 128, 0),    # 11=diningtable\n",
    "    (64, 0, 128),     # 12=dog\n",
    "    (192, 0, 128),    # 13=horse\n",
    "    (64, 128, 128),   # 14=motorbike\n",
    "    (192, 128, 128),  # 15=person\n",
    "    (0, 64, 0),       # 16=potted plant\n",
    "    (128, 64, 0),     # 17=sheep\n",
    "    (0, 192, 0),      # 18=sofa\n",
    "    (128, 192, 0),    # 19=train\n",
    "    (0, 64, 128)      # 20=tv/monitor\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98595a-79c7-4fd5-ab4c-ad8a089022e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VOC palette since 'palette' key is missing\n",
    "palette = [\n",
    "    (0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128),\n",
    "    (128, 0, 128), (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0),\n",
    "    (64, 128, 0), (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128),\n",
    "    (192, 128, 128), (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0),\n",
    "    (0, 64, 128)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca81209-859d-49a1-b605-8cf8569052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty color mask (same height & width as the output, 3 channels for RGB)\n",
    "color_mask = np.zeros((output_predictions.shape[0], output_predictions.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "# Color each pixel using the corresponding class color from the palette\n",
    "for label, color in enumerate(palette):\n",
    "    color_mask[output_predictions == label] = color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ec468-f262-4209-8883-933840d6e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, mask, and overlay\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(input_image)\n",
    "plt.axis('off')\n",
    "\n",
    "# Segmentation Mask\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Segmentation Mask\")\n",
    "plt.imshow(color_mask)\n",
    "plt.axis('off')\n",
    "\n",
    "# Overlay (original + segmentation)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(input_image)\n",
    "plt.imshow(color_mask, alpha=0.6)  # semi-transparent mask\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1301ae-e459-4cbe-8282-5f06fd2c1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aed026-5dd4-48ec-9adc-38e4af34561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(color_mask).save(\"segmentation_mask.png\")\n",
    "print(\"Segmentation mask saved as segmentation_mask.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6e35d-4e5b-48b6-ac95-b2b1469a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pillow tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8630ad-118c-4216-bf48-53ae7798487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class DeepLabSegmentation:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = deeplabv3_resnet101(pretrained=True).to(self.device).eval()\n",
    "        self.preprocess = T.Compose([\n",
    "            T.Resize((520, 520)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "        self.colors = self.generate_colors(21)\n",
    "\n",
    "    def generate_colors(self, num_classes):\n",
    "        random.seed(42)\n",
    "        return [tuple(random.choices(range(256), k=3)) for _ in range(num_classes)]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def segment(self, image: Image.Image):\n",
    "        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        output = self.model(input_tensor)[\"out\"]\n",
    "        mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "        return mask\n",
    "\n",
    "    def colorize_mask(self, mask):\n",
    "        color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "        for class_id, color in enumerate(self.colors):\n",
    "            color_mask[mask == class_id] = color\n",
    "        return Image.fromarray(color_mask)\n",
    "\n",
    "    def overlay(self, image, mask, alpha=0.6):\n",
    "        color_mask = self.colorize_mask(mask).convert(\"RGBA\")\n",
    "        base = image.convert(\"RGBA\").resize(color_mask.size)\n",
    "        blended = Image.blend(base, color_mask, alpha)\n",
    "        return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2779346-ce15-44b1-b731-2c02839956ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        model = resnet50(weights=weights)\n",
    "        self.model = torch.nn.Sequential(*list(model.children())[:-1]).to(self.device).eval()\n",
    "        self.preprocess = weights.transforms()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, image: Image.Image):\n",
    "        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        output = self.model(input_tensor).squeeze().cpu()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcdec6-ccbd-4f7c-a5fa-a69dacc6d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def process_dataset(img_dir, out_dir=\"preproc\", captions_json=None, device=None):\n",
    "    img_dir = Path(img_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    seg = DeepLabSegmentation(device=device)\n",
    "    feats = FeatureExtractor(device=device)\n",
    "\n",
    "    caption_map = {}\n",
    "    if captions_json:\n",
    "        with open(captions_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            for row in data:\n",
    "                caption_map.setdefault(row[\"image\"], []).append(row[\"caption\"])\n",
    "        elif isinstance(data, dict):\n",
    "            caption_map = {k: v if isinstance(v, list) else [v] for k, v in data.items()}\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    image_paths = [p for p in img_dir.rglob(\"*\") if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "    for img_path in tqdm(image_paths):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        mask = seg.segment(image)\n",
    "        mask_img = seg.colorize_mask(mask)\n",
    "        overlay_img = seg.overlay(image, mask)\n",
    "        feature_vector = feats(image)\n",
    "\n",
    "        stem = img_path.stem\n",
    "        mask_path = out_dir / f\"{stem}_mask.png\"\n",
    "        overlay_path = out_dir / f\"{stem}_overlay.jpg\"\n",
    "        feature_path = out_dir / f\"{stem}_feat.pt\"\n",
    "\n",
    "        mask_img.save(mask_path)\n",
    "        overlay_img.save(overlay_path)\n",
    "        torch.save(feature_vector, feature_path)\n",
    "\n",
    "        manifest.append({\n",
    "            \"image\": str(img_path),\n",
    "            \"mask\": str(mask_path.name),\n",
    "            \"overlay\": str(overlay_path.name),\n",
    "            \"features\": str(feature_path.name),\n",
    "            \"captions\": caption_map.get(img_path.name, [])\n",
    "        })\n",
    "\n",
    "    with open(out_dir / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in manifest:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Done! Outputs saved in {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4340a2d-4d54-4a3f-a362-921f6f87bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. Imports\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Tuple, Optional\n",
    "import json, random, torch, numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2. Tiny wrappers\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class DeepLabSeg:\n",
    "    \"\"\"Light wrapper around DeepLabV3-ResNet101 for fast reuse.\"\"\"\n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "        self.model = deeplabv3_resnet101(weights=weights).to(self.device).eval()\n",
    "        self.pre = weights.transforms()\n",
    "        random.seed(42)\n",
    "        self.palette = [tuple(random.choices(range(256), k=3)) for _ in range(21)]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, img: Image.Image) -> np.ndarray:\n",
    "        t = self.pre(img).unsqueeze(0).to(self.device)\n",
    "        mask = self.model(t)[\"out\"].argmax(1).squeeze().cpu().numpy()\n",
    "        return mask.astype(np.uint8)\n",
    "\n",
    "    def colorize(self, mask: np.ndarray) -> Image.Image:\n",
    "        h, w = mask.shape\n",
    "        out = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        for cid, col in enumerate(self.palette):\n",
    "            out[mask == cid] = col\n",
    "        return Image.fromarray(out)\n",
    "\n",
    "    def overlay(self, img: Image.Image, mask: np.ndarray, alpha=0.6) -> Image.Image:\n",
    "        cm = self.colorize(mask).convert(\"RGBA\")\n",
    "        return Image.blend(img.convert(\"RGBA\").resize(cm.size), cm, alpha).convert(\"RGB\")\n",
    "\n",
    "\n",
    "class ResNetFeature:\n",
    "    \"\"\"2048-D global feature from ResNet-50 (avg-pooled).\"\"\"\n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        w = ResNet50_Weights.DEFAULT\n",
    "        m = resnet50(weights=w)\n",
    "        self.model = torch.nn.Sequential(*list(m.children())[:-1]).to(self.device).eval()\n",
    "        self.pre = w.transforms()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, img: Image.Image) -> torch.Tensor:\n",
    "        t = self.pre(img).unsqueeze(0).to(self.device)\n",
    "        return self.model(t).squeeze().cpu()          # (2048,)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3. The unified pipeline\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def run_pipeline(\n",
    "    images: Union[str, Path, List[Tuple[str, Image.Image]]],\n",
    "    out_dir: Union[str, Path] = \"preproc\",\n",
    "    captions: Optional[dict] = None,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    ‚Ä¢ `images` can be\n",
    "        - a folder path\n",
    "        - a single image file path\n",
    "        - list of (name, PIL.Image) tuples (in-memory mode)\n",
    "    ‚Ä¢ `captions` optional dict  {name: [cap1, cap2, ...]}\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    seg   = DeepLabSeg(device)\n",
    "    feat  = ResNetFeature(device)\n",
    "    caps  = captions or {}\n",
    "    items = []\n",
    "\n",
    "    # -------- prepare iterable of (name, PIL.Image) --------\n",
    "    if isinstance(images, (str, Path)):\n",
    "        p = Path(images)\n",
    "        if p.is_dir():\n",
    "            for f in sorted(p.rglob(\"*\")):\n",
    "                if f.suffix.lower() in (\".jpg\", \".jpeg\", \".png\"):\n",
    "                    items.append((f.stem, Image.open(f).convert(\"RGB\")))\n",
    "        else:  # single file\n",
    "            items.append((p.stem, Image.open(p).convert(\"RGB\")))\n",
    "    else:  # already [(name, PIL.Image), ...]\n",
    "        items = images\n",
    "\n",
    "    print(f\"Processing {len(items)} image(s)‚Ä¶\")\n",
    "    manifest = []\n",
    "    for name, img in tqdm(items):\n",
    "        msk = seg(img)\n",
    "        msk_img = seg.colorize(msk)\n",
    "        ovl_img = seg.overlay(img, msk)\n",
    "        vec = feat(img)\n",
    "\n",
    "        msk_path = out_dir / f\"{name}_mask.png\"\n",
    "        ovl_path = out_dir / f\"{name}_overlay.jpg\"\n",
    "        vec_path = out_dir / f\"{name}_feat.pt\"\n",
    "\n",
    "        msk_img.save(msk_path); ovl_img.save(ovl_path); torch.save(vec, vec_path)\n",
    "\n",
    "        manifest.append({\n",
    "            \"image\": name,          # simply a key; change if you prefer path\n",
    "            \"mask\":    msk_path.name,\n",
    "            \"overlay\": ovl_path.name,\n",
    "            \"feature\": vec_path.name,\n",
    "            \"captions\": caps.get(name, []),\n",
    "        })\n",
    "\n",
    "    with open(out_dir / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.writelines(json.dumps(r)+\"\\n\" for r in manifest)\n",
    "\n",
    "    print(\"‚úÖ Done!  Outputs in ‚Üí\", out_dir.resolve())\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4. Example calls\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# ‚Ä¢ Entire folder on CPU:\n",
    "# run_pipeline(\"images/\", out_dir=\"preproc_cpu\", device=\"cpu\")\n",
    "\n",
    "# ‚Ä¢ Just one file:\n",
    "# run_pipeline(\"images/cat.jpg\", out_dir=\"preproc_single\")\n",
    "\n",
    "# ‚Ä¢ In-memory mode:\n",
    "# imgs_mem = [(\"cat01\", img1), (\"dog02\", img2)]\n",
    "# run_pipeline(imgs_mem, out_dir=\"preproc_mem\", device=\"cpu\",\n",
    "#              captions={\"cat01\": [\"A cute cat.\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecfa66-e032-4f95-bb00-4c75b311380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, itertools, pprint, pathlib\n",
    "pp = pprint.PrettyPrinter(depth=2)\n",
    "\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "pp.pprint(rows[:3])          # quick peek\n",
    "print(\"Total samples:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8449480-4dd6-4af3-ae48-028c92ecaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionSet(Dataset):\n",
    "    def __init__(self, manifest, vocab):\n",
    "        self.manifest = manifest\n",
    "        self.vocab    = vocab       # word ‚Üî id converter\n",
    "\n",
    "    def __len__(self): return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.manifest[idx]\n",
    "        # Load the 2048-D CNN feature vector\n",
    "        feat = torch.load(\"preproc/\" + row[\"feature\"])\n",
    "        # Pick ONE caption at random if multiple exist\n",
    "        caption = row[\"captions\"][0] if row[\"captions\"] else \"\"\n",
    "        # Numericalise caption ‚Üí tensor of token IDs\n",
    "        cap_ids = torch.tensor([self.vocab[\"<start>\"]] +\n",
    "                               [self.vocab.get(w, self.vocab[\"<unk>\"]) for w in caption.lower().split()] +\n",
    "                               [self.vocab[\"<end>\"]])\n",
    "        return feat, cap_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b53dc6-263e-4dd8-8c5e-8253be12420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Load the manifest\n",
    "with open(\"preproc/manifest.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = [json.loads(line) for line in f]\n",
    "\n",
    "# Build vocabulary\n",
    "words = Counter(itertools.chain.from_iterable(\n",
    "    cap.lower().split() for row in rows for cap in row[\"captions\"]\n",
    "))\n",
    "\n",
    "special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "itos = special + [w for w, n in words.items() if n >= 2]  # keep frequent words\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "# Define vocab as dictionary (stoi), and create vocab_size\n",
    "vocab = stoi\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d939a6c-ed19-4a7b-8a95-ab7eb1bf2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=vocab_size):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb    = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm   = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        \"\"\"feats: (B,2048)  caps: (B,T)\"\"\"\n",
    "        h0 = torch.tanh(self.fc_img(feats)).unsqueeze(0)  # (1,B,H)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        emb = self.emb(caps)\n",
    "        out, _ = self.lstm(emb, (h0, c0))\n",
    "        logits = self.fc_out(out)  # (B,T,V)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63161b0-8106-4cd9-8927-9914f6521235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, max_len=20):\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                for cap in entry[\"captions\"]:\n",
    "                    self.data.append((entry[\"features\"], cap))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        words = caption.lower().split()\n",
    "        tokens = [\"<start>\"] + words[:self.max_len - 2] + [\"<end>\"]\n",
    "        ids = [self.vocab.get(w, self.vocab[\"<unk>\"]) for w in tokens]\n",
    "        pad_len = self.max_len - len(ids)\n",
    "        return torch.tensor(ids + [self.vocab[\"<pad>\"]] * pad_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_path, caption = self.data[idx]\n",
    "        feat = torch.load(feat_path)  # (2048,)\n",
    "        cap_tensor = self.encode_caption(caption)\n",
    "        return feat, cap_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecc9b9-0602-44e1-941d-184b673ce6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a737f-5120-4cf8-b4c5-9f3da2496919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "image_dir = Path(\"images/\")\n",
    "image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "print(f\"Found {len(image_files)} image(s).\")\n",
    "for img in image_files[:5]:\n",
    "    print(\"-\", img.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa197ca-b77e-4b19-82a0-b45aac5b341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    img_dir=\"images/\",         # ‚úÖ Put the correct folder path here!\n",
    "    out_dir=\"preproc/\",        \n",
    "    captions_json=None,        # Or provide your JSON if available\n",
    "    device=\"cpu\"               # or \"cuda\" if you use GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b7b08-98b8-4f99-b967-c4d8df1f8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "print(\"‚úÖ Total samples:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821fe9-1ceb-4e57-9d23-d35048ae15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    img_dir=\".\",              # ‚úÖ This points to the current folder where your .jpg is\n",
    "    out_dir=\"preproc/\",\n",
    "    captions_json=None,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401456df-bbe0-4e66-88f6-826146aa98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "print(\"‚úÖ Total usable samples:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefee7a-95b5-4d4c-b3e4-2a8152abdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = \"photo_2025-06-09_21-10-49.jpg\"\n",
    "try:\n",
    "    img = Image.open(img_path)\n",
    "    img.verify()\n",
    "    print(\"‚úÖ Image is valid and readable.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error loading image:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0859f23-ced9-4937-9bcc-b131e45c1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in tqdm(img_files):\n",
    "    print(\"üñºÔ∏è Found:\", img_path)\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to open {img_path}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc7e03-d5f7-4711-ac00-ecae72582291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "def process_dataset(img_dir, out_dir, captions_json=None, device=\"cpu\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    model = resnet50(pretrained=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get list of images\n",
    "    supported_exts = (\".jpg\", \".jpeg\", \".png\")\n",
    "    img_files = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith(supported_exts)]\n",
    "\n",
    "    print(f\"üîç Found {len(img_files)} images in {img_dir}\")\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    for img_path in tqdm(img_files):\n",
    "        print(\"üñºÔ∏è Checking:\", img_path)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = model(image_tensor).squeeze().cpu().numpy().tolist()\n",
    "\n",
    "        sample = {\n",
    "            \"image\": os.path.abspath(img_path),\n",
    "            \"features\": features,\n",
    "            \"caption\": None  # Will be filled later if captions_json provided\n",
    "        }\n",
    "        manifest.append(sample)\n",
    "\n",
    "    print(f\"‚úÖ Total usable samples: {len(manifest)}\")\n",
    "\n",
    "    # Save manifest\n",
    "    with open(os.path.join(out_dir, \"manifest.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in manifest:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    print(f\"üì¶ Manifest saved to {out_dir}/manifest.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dae5a-b658-4bff-a3c0-48ec1223cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    img_dir=\".\",              # since your image is in current folder\n",
    "    out_dir=\"preproc/\",\n",
    "    captions_json=None,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837718c8-4e82-4bf8-bb8e-759bad4e5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Helper tokenizer\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "# Build vocab from dummy captions or predefined words\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=1):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            tokens = tokenize(sentence)\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = tokenize(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b93b8-f86a-486b-9465-62ef74cca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab):\n",
    "        with open(manifest_path, 'r') as f:\n",
    "            self.samples = [json.loads(line) for line in f]\n",
    "        self.vocab = vocab\n",
    "        self.max_len = 20  # cap caption length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        caption = sample.get(\"caption\") or \"a sample image\"\n",
    "        caption_idxs = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption)[:self.max_len] + [self.vocab.stoi[\"<EOS>\"]]\n",
    "        padded = caption_idxs + [self.vocab.stoi[\"<PAD>\"]] * (self.max_len + 2 - len(caption_idxs))\n",
    "        return torch.tensor(sample[\"features\"], dtype=torch.float32), torch.tensor(padded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1010ee-d21a-4cd0-911d-fda9c61709dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Build dummy captions if missing\n",
    "dummy_captions = [\"a segmented image\", \"an example picture\"]\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(dummy_captions)\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b75395-3aff-4bb9-bdf8-92c248e78c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, transform=None):\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.samples = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        img = Image.open(sample['image']).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Use dummy caption if not available\n",
    "        caption = sample.get('caption', \"a photo\")\n",
    "        tokens = self.vocab.tokenize(caption)\n",
    "        caption_ids = [self.vocab.start_token_id] + self.vocab.encode(tokens) + [self.vocab.end_token_id]\n",
    "\n",
    "        return img, torch.tensor(caption_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30eee98-7ebf-4d0c-aac8-821f00ecee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # Encode image\n",
    "        img_feats = self.fc_img(img_feats).unsqueeze(1)  # (B, 1, H)\n",
    "        embeds = self.embedding(captions)  # (B, T, E)\n",
    "        x = torch.cat([img_feats, embeds], dim=1)  # (B, T+1, E)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6006cb-3d36-4b09-a8f7-010d5330a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Collate function to pad caption sequences in a batch.\"\"\"\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    # Stack images (they're already tensors)\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Pad captions\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=0)  # 0 = <PAD> token\n",
    "\n",
    "    return images, captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d4afb-6482-4738-84a8-79a9e1e522e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609012e-2ed1-4e8c-ab8f-c002d6ef5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        # img_feat: (B, feat_dim)\n",
    "        h0 = self.fc_img(img_feat).unsqueeze(0)  # (1, B, H)\n",
    "        x = self.embed(captions)                 # (B, T, E)\n",
    "        out, _ = self.rnn(x, h0)                 # (B, T, H)\n",
    "        return self.fc_out(out)                  # (B, T, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90c29c-efef-4b8a-9e8f-772995e0c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Captioner(vocab_size=len(vocab)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ebcb5-fcd4-40c7-a863-8d218df7cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff3766-7e8a-45fb-91c5-7c9e76251909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Captioner(vocab_size=len(vocab.itos)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d094ee7-30d4-4524-94c7-e0e2d1339c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Captioner(vocab_size=len(vocab.itos)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad90d5e-34e7-4e16-9532-c106a553adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccdba0d-ef09-4e09-b869-e6d8997d453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Captioner(vocab_size=len(vocab.itos)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6136067-aa73-4770-9e64-b77af97d0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 0.  Imports\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import json, re, random, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1.  Vocabulary with special tokens built-in\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        # special tokens FIRST so their indices are fixed\n",
    "        self.special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.itos = self.special.copy()      # index ‚Üí string\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.special)}\n",
    "        self.min_freq = min_freq\n",
    "        self._built = False\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "    def build(self, caption_list):\n",
    "        freqs = Counter(t for sent in caption_list for t in self._tokenize(sent))\n",
    "        for word, n in freqs.items():\n",
    "            if n >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "        self._built = True\n",
    "\n",
    "    # usable helpers\n",
    "    def encode(self, text):\n",
    "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in self._tokenize(text)]\n",
    "    def decode(self, idxs):\n",
    "        return \" \".join(self.itos[i] for i in idxs if i not in (self.stoi[\"<pad>\"], self.stoi[\"<start>\"], self.stoi[\"<end>\"]))\n",
    "    def __len__(self): return len(self.itos)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2.  Manifest & captions ‚Üí build vocab\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MANIFEST = Path(\"preproc/manifest.jsonl\")\n",
    "assert MANIFEST.exists(), \"Run process_dataset() first!\"\n",
    "\n",
    "rows = [json.loads(l) for l in MANIFEST.open()]\n",
    "all_caps = []\n",
    "for r in rows:\n",
    "    # FALL-BACK caption if none present\n",
    "    if not r.get(\"captions\"):\n",
    "        r[\"captions\"] = [\"a photo\"]\n",
    "    all_caps.extend(r[\"captions\"])\n",
    "\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build(all_caps)\n",
    "print(\"‚úÖ Vocab size:\", len(vocab))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3.  Dataset & DataLoader (features + captions)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_rows, vocab, max_len=20):\n",
    "        self.rows, self.vocab, self.max_len = manifest_rows, vocab, max_len\n",
    "\n",
    "    def __len__(self): return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.rows[idx]\n",
    "        # load 2048-D feature vector saved by process_dataset()\n",
    "        feat  = torch.load(Path(\"preproc\") / row[\"feature\"])   # (2048,)\n",
    "        # choose one caption at random if multiple exist\n",
    "        cap   = random.choice(row[\"captions\"])\n",
    "        ids   = (\n",
    "            [self.vocab.stoi[\"<start>\"]]\n",
    "            + self.vocab.encode(cap)[: self.max_len]\n",
    "            + [self.vocab.stoi[\"<end>\"]]\n",
    "        )\n",
    "        return feat, torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def collate(batch):\n",
    "    feats, caps = zip(*batch)\n",
    "    feats = torch.stack(feats)            # (B,2048)\n",
    "    caps  = pad_sequence(caps, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n",
    "    return feats, caps\n",
    "\n",
    "train_ds = CaptionDataset(rows, vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate)\n",
    "print(\"‚úÖ Dataset size:\", len(train_ds))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4.  Simple Captioner model (features ‚Üí GRU decoder)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=512, vocab_size=len(vocab)):\n",
    "        super().__init__()\n",
    "        self.img_fc = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb    = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab.stoi[\"<pad>\"])\n",
    "        self.gru    = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        # feats: (B,2048); caps: (B,T)\n",
    "        h0 = torch.tanh(self.img_fc(feats)).unsqueeze(0)  # (1,B,H)\n",
    "        x  = self.emb(caps)                               # (B,T,E)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        return self.fc_out(out)                           # (B,T,V)\n",
    "\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model    = Captioner().to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5.  Training loop (few epochs demo)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EPOCHS = 5\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0\n",
    "    for feats, caps in train_dl:\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(feats, caps[:, :-1])                      # teacher forcing\n",
    "        loss   = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            caps[:, 1:].reshape(-1)\n",
    "        )\n",
    "        loss.backward(); optimiser.step()\n",
    "        running += loss.item()\n",
    "    print(f\"Epoch {ep}/{EPOCHS} ‚Äî loss: {running / len(train_dl):.4f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 6.  Quick inference helper\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_caption(feat_tensor, max_len=20):\n",
    "    model.eval()\n",
    "    feat_tensor = feat_tensor.to(device).unsqueeze(0)     # (1,2048)\n",
    "    with torch.no_grad():\n",
    "        hidden = torch.tanh(model.img_fc(feat_tensor)).unsqueeze(0)\n",
    "        word   = torch.tensor([[vocab.stoi[\"<start>\"]]], device=device)\n",
    "        out_words = []\n",
    "        for _ in range(max_len):\n",
    "            emb = model.emb(word)\n",
    "            out, hidden = model.gru(emb, hidden)\n",
    "            next_id = model.fc_out(out[:, -1]).argmax(-1).item()\n",
    "            if next_id == vocab.stoi[\"<end>\"]: break\n",
    "            out_words.append(vocab.itos[next_id])\n",
    "            word = torch.tensor([[next_id]], device=device)\n",
    "    return \" \".join(out_words)\n",
    "\n",
    "#  pick first sample to test\n",
    "test_feat, _ = train_ds[0]\n",
    "print(\"üìù Sample caption:\", generate_caption(test_feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728c142-cfed-4b27-bc79-0e17b8a8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "image_root = pathlib.Path(\"preproc/images\")\n",
    "out_dir = pathlib.Path(\"preproc/features\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load manifest\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "\n",
    "# Load pretrained ResNet50\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))  # remove final FC\n",
    "model.eval()\n",
    "\n",
    "transform = ResNet50_Weights.DEFAULT.transforms()\n",
    "\n",
    "for row in tqdm(rows):\n",
    "    try:\n",
    "        img_path = image_root / row[\"image\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0)   # (1,3,224,224)\n",
    "        with torch.no_grad():\n",
    "            feat = model(x).squeeze()     # (2048,)\n",
    "        # save feature vector\n",
    "        feat_file = f\"{img_path.stem}.pt\"\n",
    "        torch.save(feat, out_dir / feat_file)\n",
    "        # add feature path to row\n",
    "        row[\"feature\"] = f\"features/{feat_file}\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {row['image']}: {e}\")\n",
    "\n",
    "# Overwrite manifest with new feature keys\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    for row in rows:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"‚úÖ Features saved & manifest updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bb5f4-8333-4497-95d5-2e4f634af24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transforms again (if not already)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9003d2d-fbcb-4516-8a63-b84f37ab40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Captioner(vocab_size=len(vocab.itos)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi.get(\"<pad>\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc21b2-3a49-4a37-bb52-933dbbfaaaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for feats, caps in train_dl:\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(feats, caps[:, :-1])  # input: all tokens except last\n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[-1]), caps[:, 1:].reshape(-1))  # target: all tokens except first\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_dl)\n",
    "    print(f\"üìò Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a72499-30de-4fb4-8eb3-8b4801c5d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, vocab, transform=None):\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        # ‚úÖ Properly load and parse each line as a JSON object (dict)\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            self.rows = [json.loads(line) for line in f]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        \n",
    "        # ‚úÖ Load precomputed image feature\n",
    "        feat_path = Path(\"preproc\") / row[\"feature\"]\n",
    "        feat = torch.load(feat_path)  # should be shape (2048,)\n",
    "        \n",
    "        # ‚úÖ Pick one caption randomly\n",
    "        caption = random.choice(row[\"captions\"]).lower()\n",
    "        \n",
    "        # ‚úÖ Tokenize and convert to indices\n",
    "        tokens = [\"<start>\"] + caption.split() + [\"<end>\"]\n",
    "        token_ids = [self.vocab.stoi.get(token, self.vocab.stoi[\"<unk>\"]) for token in tokens]\n",
    "        cap_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        return feat, cap_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e94c1-4686-4cb4-8740-9900a4158e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186859bb-8012-49cb-ac1d-7c8b68410413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Captioner model definition\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        img_feat = self.fc_img(img_feat).unsqueeze(1)  # (B, 1, H)\n",
    "        cap_embeds = self.embed(captions)              # (B, T, E)\n",
    "        x = torch.cat([img_feat, cap_embeds[:, :-1, :]], dim=1)  # Shifted input\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model, optimizer, loss\n",
    "vocab_size = len(vocab.itos)\n",
    "model = Captioner(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "\n",
    "print(f\"‚úÖ Model ready on {device} with vocab size {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a07223-6442-44a7-9981-1d8f4e736d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Captioner()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # Set PAD_IDX appropriately\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac60cf-8790-4a4f-8a90-bc4637574d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        img_feat = self.fc_img(img_feat).unsqueeze(1)         # (B, 1, H)\n",
    "        cap_embeds = self.embed(captions)                     # (B, T, E)\n",
    "        x = torch.cat([img_feat, cap_embeds[:, :-1, :]], dim=1)  # Shifted input\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a25fc6-1804-4621-94d4-87011d6a76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "PAD_IDX = 0  # Replace with your actual padding token index\n",
    "model = Captioner()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ca66a-f4f6-4edf-a8f2-498afa1a892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # You can increase this\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for img_feats, captions in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(img_feats, captions)  # (B, T, V)\n",
    "\n",
    "        # Shift targets to match the output\n",
    "        targets = captions[:, 1:]  # (B, T-1)\n",
    "        outputs = outputs[:, :-1, :]  # (B, T-1, V)\n",
    "\n",
    "        # Reshape for loss\n",
    "        outputs = outputs.reshape(-1, outputs.size(2))  # (B*(T-1), V)\n",
    "        targets = targets.reshape(-1)                  # (B*(T-1))\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7cb4c-2f23-46f7-9c91-f235ebfa1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ad26b-7f47-4a29-b6e0-3aac1d4f54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2b2a6-44a4-42ad-8eea-243a900c23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4aec6d-ee94-4148-9400-91010b191ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet18 and remove the classifier layer\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "resnet.eval()\n",
    "\n",
    "# Transform image to fit ResNet input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load your image\n",
    "img_path = \"photo_2025-06-09_21-10-49.jpg\"  # file already in your folder\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = transform(img).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    features = resnet(img).squeeze().flatten()  # Shape: (512,)\n",
    "\n",
    "# Convert to dataset-style tensor\n",
    "img_features = features.unsqueeze(0)  # Shape: (1, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef7b8f-c540-471d-a8ca-b2c067165032",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_captions = torch.tensor([[1, 5, 10, 3]])  # Example: [<start>, 'a', 'cat', <end>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6acf4-d2e9-4df5-ab1e-1295f5046e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf2b91-da76-4f92-8289-983ce779cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.fc = nn.Linear(512, 1000)  # change 1000 to vocab size\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        return self.fc(img_feats)  # dummy forward for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c25eb-1101-469c-a611-4d80f2f59a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Captioner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f1bcf-6abc-46de-8585-b4866ff1e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870619-ec92-4b48-9cfa-8ce6aaeb7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa050a95-c14f-4c23-aba6-9fa1347e85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0000386-b51b-4122-9f80-3cbe3603f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for img_feats, captions in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img_feats, captions)\n",
    "\n",
    "        # Flatten and reshape outputs + targets here if needed\n",
    "        loss = criterion(outputs, captions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3e708-52cb-482b-bd2b-3fd2e4e71c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_feats, captions in train_dl:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(img_feats, captions)  # (B, T, V) ‚Äî e.g. (32, 20, 1000)\n",
    "\n",
    "    # Reshape for CrossEntropyLoss\n",
    "    outputs = outputs.view(-1, outputs.size(-1))      # (B*T, V)\n",
    "    targets = captions.view(-1)                       # (B*T)\n",
    "\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8acc5-b118-43d1-a652-eab16dd2a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"outputs shape:\", outputs.shape)  # should be (B*T, V)\n",
    "print(\"targets shape:\", targets.shape)  # should be (B*T,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cdb18-0c42-4cc3-9fe6-cd619df37add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # img_feats: (batch, feature_dim)\n",
    "        # captions: (batch, seq_len)\n",
    "\n",
    "        embeddings = self.embedding(captions)  # (batch, seq_len, embed_size)\n",
    "\n",
    "        # You can optionally expand and feed image features as first input token\n",
    "        # or use it to initialize hidden state\n",
    "\n",
    "        outputs, _ = self.lstm(embeddings)     # (batch, seq_len, hidden_size)\n",
    "        outputs = self.fc(outputs)             # (batch, seq_len, vocab_size)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71ce72-f714-4cc2-b9a8-87f54f59e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(img_feats, captions)   # (B, T, V)\n",
    "outputs = outputs[:, 1:, :]            # skip <start> prediction if needed\n",
    "targets = captions[:, 1:]              # shift ground truth\n",
    "\n",
    "outputs = outputs.reshape(-1, outputs.shape[-1])  # (B*T, V)\n",
    "targets = targets.reshape(-1)                     # (B*T,)\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82658621-78ca-410e-9fdc-13a7d8d81f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        \"\"\"\n",
    "        img_feats is unused here but you can use it to init hidden state\n",
    "        captions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(captions)                 # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(embeddings)                   # (B, T, H)\n",
    "        outputs = self.linear(lstm_out)                       # (B, T, V)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121e7b9-3139-4860-85fc-49c90ff2f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(img_feats, captions)        # (B, T, V)\n",
    "outputs = outputs[:, 1:, :]                 # skip <start> token\n",
    "targets = captions[:, 1:]                   # predict next word\n",
    "\n",
    "outputs = outputs.reshape(-1, outputs.size(-1))  # (B*T, V)\n",
    "targets = targets.reshape(-1)                   # (B*T)\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a187a4a-d7dc-473a-a1bd-b67789466772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # Use img_feats later if needed for context initialization\n",
    "        embeddings = self.embedding(captions)            # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(embeddings)              # (B, T, H)\n",
    "        outputs = self.linear(lstm_out)                  # (B, T, V)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb52fd-5413-4de0-b597-e23bbd47ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # img_feats is not used in this simple version; can be added to init LSTM hidden\n",
    "        embeddings = self.embedding(captions)         # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(embeddings)           # (B, T, H)\n",
    "        outputs = self.linear(lstm_out)               # (B, T, V)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0be9d3-c223-4011-8778-d97957a4401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [[\"a\", \"dog\", \"on\", \"a\", \"beach\"], [\"a\", \"cat\", \"on\", \"a\", \"mat\"], ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd5345-5661-4290-b927-456cab1d2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\n",
    "    \"<START> a dog is playing with a ball <END>\",\n",
    "    \"<START> a child is eating ice cream <END>\",\n",
    "    \"<START> a man is riding a bicycle <END>\"\n",
    "]\n",
    "\n",
    "with open(\"captions.txt\", \"w\") as f:\n",
    "    for caption in captions:\n",
    "        f.write(caption + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71de87c-932e-4552-b80c-d93b8f2681f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "\n",
    "with open(\"captions.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # skip empty lines\n",
    "            tokens = line.lower().split()  # basic tokenization\n",
    "            captions.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeac97f-a3e5-4a7e-8ac7-8913ffcf8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten all tokens\n",
    "all_tokens = [token for caption in captions for token in caption]\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Set a threshold frequency (e.g., keep words appearing ‚â•1 time)\n",
    "threshold = 1\n",
    "vocab = [word for word, count in counter.items() if count >= threshold]\n",
    "\n",
    "# Special tokens\n",
    "vocab = ['<PAD>', '<START>', '<END>', '<UNK>'] + sorted(set(vocab))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d72f9-eb04-4763-b03c-340b499cca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(cap) for cap in captions)\n",
    "\n",
    "encoded_captions = []\n",
    "for caption in captions:\n",
    "    encoded = [word2idx.get(word, word2idx['<UNK>']) for word in caption]\n",
    "    # Pad to max_len\n",
    "    encoded += [word2idx['<PAD>']] * (max_len - len(encoded))\n",
    "    encoded_captions.append(encoded)\n",
    "\n",
    "encoded_captions = torch.tensor(encoded_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4f9e0-7081-4331-9753-bc808cff0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "# Assume img_features is a torch.Tensor\n",
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794b352-25de-41a4-b6ce-30e02b35311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "model = Captioner(embed_size, hidden_size, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc305663-a0ad-4989-8a8b-b7e2f727e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image features shape:\", img_features.shape)\n",
    "print(\"Encoded captions shape:\", encoded_captions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ebb45-d4dd-4b40-b8b8-18621c8b45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix shape mismatch: make image features match captions\n",
    "img_features = img_features.repeat(3, 1)  # Now shape is [3, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38db955-23ee-4856-bd74-e04e021b2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=2, shuffle=True)  # batch_size can be 2 or 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ddb8a-59ce-4821-95d4-587e98ff30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # you can increase later if needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for img_feats, captions in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(img_feats, captions)       # Shape: (B, T, V)\n",
    "        outputs = outputs[:, 1:, :]                 # Skip <START>\n",
    "        targets = captions[:, 1:]                   # Next word prediction\n",
    "\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))  # (B*T, V)\n",
    "        targets = targets.reshape(-1)                    # (B*T)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef38083-ca4d-4d07-b1a1-736c2e573e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)  # (1, T)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)  # (1, T, V)\n",
    "            next_word_logits = output[0, -1]  # last token's output\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "\n",
    "            caption.append(predicted)\n",
    "\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "\n",
    "    # Decode\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab[idx] for idx in caption[1:-1]])  # Skip <START> and <END>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527b3b7-cbc7-4504-bcdd-c28ece68efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten all tokens\n",
    "all_tokens = [token for caption in captions for token in caption]\n",
    "\n",
    "# Count frequency\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Set a threshold if needed, e.g., min_freq = 1\n",
    "tokens = sorted(counter)\n",
    "\n",
    "# Build vocab dict\n",
    "vocab = {token: idx + 2 for idx, token in enumerate(tokens)}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<START>'] = 1\n",
    "vocab['<END>'] = len(vocab)  # or any fixed value\n",
    "\n",
    "# OPTIONAL: Create inverse vocab for decoding\n",
    "inv_vocab = {idx: token for token, idx in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa072f3-94aa-4685-8c1a-3d4e531470e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption(model, img_features[0], vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf8fd0-2b98-473e-b4b1-90e4ea11d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<UNK>'] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a2f26-ed1e-4681-a5fb-106e882d7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "# Safely decode even unknown tokens\n",
    "return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77926758-6636-4b3b-aacc-f3c6ffb48872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)  # (1, T)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)  # (1, T, V)\n",
    "            next_word_logits = output[0, -1]  # last token's output\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "\n",
    "    # Create reverse vocab\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "    # ‚úÖ This line must be inside the function!\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cb86f-e67b-4feb-8def-90b6d4301ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption(model, img_features[0], vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24127591-be5f-428e-a562-279424c610b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)  # shape (1, T)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)  # shape (1, T, V)\n",
    "            next_word_logits = output[0, -1]  # last timestep\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "\n",
    "    # Decode caption (skip <START> and <END>)\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e95460-4045-4025-b131-c45a24affa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption(model, img_features[0], vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fd6d2-42b4-4ee6-b228-7f8f3e809644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_features)):\n",
    "    caption = generate_caption(model, img_features[i], vocab)\n",
    "    print(f\"Image {i+1} Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff76e4-5c31-47be-906b-7d52698b70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'caption_model.pth')\n",
    "\n",
    "# Load\n",
    "model.load_state_dict(torch.load('caption_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734e744-325f-4685-b22d-574945b0672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c992ae-dad2-4d78-877b-59f8ef12e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# === Load your model and vocab ===\n",
    "from model import Captioner  # Replace with actual model import\n",
    "from utils import load_vocab, extract_features, generate_caption  # You may need to define/import these\n",
    "\n",
    "# Load model\n",
    "model = Captioner(embed_size=256, hidden_size=512, vocab_size=your_vocab_size)\n",
    "model.load_state_dict(torch.load(\"caption_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Load vocab\n",
    "vocab = load_vocab(\"vocab.pkl\")  # Load the vocab dictionary you saved\n",
    "\n",
    "# === Define image preprocessing ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "st.title(\"üñºÔ∏è Image Caption Generator\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "    # Preprocess and extract features\n",
    "    img_tensor = transform(image).unsqueeze(0)\n",
    "    img_feat = extract_features(img_tensor)  # Your pre-trained feature extractor\n",
    "\n",
    "    # Generate Caption\n",
    "    caption = generate_caption(model, img_feat.squeeze(0), vocab)\n",
    "    st.markdown(f\"**Generated Caption:** {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593e6c9-4e48-49d5-8324-cbb2bbbf891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        embeddings = self.embed(captions)  # (B, T, E)\n",
    "        img_feat = img_feat.unsqueeze(1)   # (B, 1, E)\n",
    "        embeddings = torch.cat((img_feat, embeddings), 1)  # (B, T+1, E)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9583f-4cf2-498c-84d7-aacf3eb26dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import pickle\n",
    "\n",
    "def extract_features(img_tensor):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.squeeze(0)\n",
    "\n",
    "def load_vocab(path='vocab.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)\n",
    "            next_word_logits = output[0, -1]\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5820cd9-9be1-4791-9c1a-649815b68ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import pickle\n",
    "\n",
    "def extract_features(img_tensor):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.squeeze(0)\n",
    "\n",
    "def load_vocab(path='vocab.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)\n",
    "            next_word_logits = output[0, -1]\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffea43-e1d5-4107-84f0-769755273e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        embeddings = self.embed(captions)  # (B, T, E)\n",
    "        img_feat = img_feat.unsqueeze(1)   # (B, 1, E)\n",
    "        embeddings = torch.cat((img_feat, embeddings), 1)  # (B, T+1, E)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc0151-8c37-4574-aa6a-18d5737c56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"captions.txt\", \"w\") as f:\n",
    "    f.write(\"image1.jpg A man riding a bike.\\n\")\n",
    "    f.write(\"image2.jpg A cat sleeping on the bed.\\n\")\n",
    "    f.write(\"image3.jpg A group of people playing football.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f22cb-7e94-4475-9e25-beee2c16494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    image_name, caption = line.strip().split(' ', 1)\n",
    "    print(f\"Image: {image_name}, Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de14d4-1c61-46b7-8cb6-da685949f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Read captions\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "captions = [line.strip().split(' ', 1)[1] for line in lines]\n",
    "\n",
    "# Tokenize and count words\n",
    "all_words = []\n",
    "for caption in captions:\n",
    "    all_words.extend(caption.lower().split())\n",
    "\n",
    "# Count word frequency\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Optionally, create word2idx and idx2word\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243554e2-58b1-4285-8148-abafc8dd4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "for line in lines:\n",
    "    image_name, caption = line.strip().split(' ', 1)\n",
    "    image_path = os.path.join(\"images\", image_name)  # replace \"images\" with your folder path\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        img.show()  # just to verify it loads\n",
    "    else:\n",
    "        print(f\"Image {image_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93a990-0e32-461b-8648-2c4d85105afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(\"images\", image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c42a7-3306-4729-a7a5-7c4d56624a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Read captions from file\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if \"|\" not in line:\n",
    "        continue  # skip malformed lines\n",
    "    image_name, caption = line.strip().split('|', 1)\n",
    "    image_path = os.path.join(\"images\", image_name)\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        print(f\"\\n‚úÖ Showing: {image_name} - {caption}\")\n",
    "        img = Image.open(image_path)\n",
    "        img.show()\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Image {image_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a538d-f98f-445d-8a44-23dcd3d4078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
